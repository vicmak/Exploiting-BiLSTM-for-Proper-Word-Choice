from collections import Counter, defaultdict
import re
from itertools import count
import torch
from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM
import random
import time

import numpy as np
import nltk
from nltk.corpus import wordnet as wn

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()

class Vocab:
    def __init__(self, w2i=None):
        if w2i is None: w2i = defaultdict(count(0).next)
        self.w2i = dict(w2i)
        self.i2w = {i:w for w,i in w2i.iteritems()}
    @classmethod
    def from_corpus(cls, corpus):
        w2i = defaultdict(count(0).next)
        for sent in corpus:
            [w2i[word] for word in sent]
        return Vocab(w2i)

    def size(self): return len(self.w2i.keys())


def read(fname):

    count = 0
    with file(fname) as fh:
        for line in fh:
            count = count + 1
            sent = line.lower().strip().split()
            #sent.append("<stop>")
            sent = ["<start>"] + sent + ["<stop>"]
            #sent.reverse()
            #scientific count <= 2775000
            #coca: count <= 4800000
            if count <= 2775000 and len(sent) <=42:
                yield sent


#train = list(read(train_file))

#words = []
#wc = Counter()
#for sent in train:
#    for w in sent:
#        words.append(w)
#        wc[w] += 1

#vw = Vocab.from_corpus([words])
##STOP = vw.w2i["<stop>"]
#START = vw.w2i["<start>"]
#nwords = vw.size()

#LAYERS = 1
#INPUT_DIM = 200 #50  #256
#HIDDEN_DIM = 200 # 50  #1024
#print "words", nwords

#dy.init()
#model = dy.Model()
#trainer = dy.AdamTrainer(model)

#WORDS_LOOKUP = model.add_lookup_parameters((nwords, INPUT_DIM))

#W_sm = model.add_parameters((nwords, HIDDEN_DIM * 2))
#b_sm = model.add_parameters(nwords)

#builders = [
#        LSTMBuilder(LAYERS, INPUT_DIM, HIDDEN_DIM, model),
#        LSTMBuilder(LAYERS, INPUT_DIM, HIDDEN_DIM, model),
#        ]


def evaluate_sentence(sentence):

    print sentence
    mid_index = get_check_index(sentence)
    prefix = get_prefix(sentence, mid_index)
    postfix = get_postfix(sentence, mid_index)
    original_word = get_original_word(sentence, mid_index)
    corrected_word = get_corrected_word(sentence, mid_index)

    predictions = get_predictions(prefix, postfix, original_word)
    mrr = get_mrr(predictions, corrected_word)
    #log_sentence(sentence, predictions, mrr)
    return mrr


def ExtractAlphanumeric(ins):
    from string import ascii_letters, digits, whitespace, punctuation
    return "".join([ch for ch in ins if ch in (ascii_letters + whitespace + punctuation)])


def get_predictions(prefix, postfix, original_word):

    print "prefix", prefix
    print "postfix", postfix
    print "Original Word", original_word

    result = []


    text = "[CLS] " + " ".join(prefix) + " [MASK] " + " ".join(postfix) + " [SEP]"

    utf8_apostrophe = b'\xe2\x80\x99'.decode("utf8")
    string_apostrophe = "'"

    print text
    text = ExtractAlphanumeric(text)
    #re.sub(utf8_apostrophe, string_apostrophe, text)
    #print text
    text = text.decode('utf-8')
    #print text

    tokenized_text = tokenizer.tokenize(text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)

    # Create the segments tensors.
    segments_ids = [0] * len(tokenized_text)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Predict all tokens
    with torch.no_grad():
        predictions = model(tokens_tensor, segments_tensors)

    top_k = 100
    probs = torch.nn.functional.softmax(predictions[0, len(prefix)+1], dim=-1)
    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)

    for i, pred_idx in enumerate(top_k_indices):
        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx.item()])[0]
        result.append(predicted_token)

    return result

def get_mrr(predictions, correct_word):
    rank = 1
    start_index = 0

    while start_index < len(predictions):
        if correct_word != predictions[start_index]:
            rank = rank + 1
        else:
            break
        start_index = start_index + 1

    if start_index == len(predictions):
        return 0
    return 1.0/float(rank)



def clean_text(token):
    token = token.replace(",", "")
    token = token.replace("-", " ")
    token = token.replace(".", "")
    token = token.replace("\'", "")
    token = token.replace("?", "")
    token = token.replace(":", "")
    token = token.replace(";", "")
    token = token.replace("\"", "")
    token = token.replace(")", "")
    token = token.replace("(", "")
    token = token.replace("[", "")
    token = token.replace("]", "")
    token = token.replace("}", "")
    token = token.replace("{", "")
    return token


def load_sentences(filename):
    sents = []
    with open(filename) as f:
        for line in f:
            line = clean_text(line)
            line = line.lower().strip()
            tokens = line.split()
            sents.append(tokens)
           #print tokens
    return sents


def get_check_index(sent):
    for i in xrange(len(sent)):
        if '*' in sent[i]:
            return i
    return -1


def get_prefix(sent, mid_index):
    prefix = []
    for i in xrange(len(sent)):
        if i < mid_index:
            prefix.append(sent[i])
    return prefix


def get_postfix(sent, mid_index):
    postfix = []
    for i in xrange(len(sent)):
        if i > mid_index:
            postfix.append(sent[i])
    return postfix


def get_original_word(sent, mid_index):
    mid_word = sent[mid_index]
    split_words = mid_word.split("/")
    original_word = split_words[0][1:len(split_words[0])]
    return original_word


def get_corrected_word(sent, mid_index):
    mid_word = sent[mid_index]
    split_words = mid_word.split("/")
    print  mid_word, mid_index
    corrected_word = split_words[1]
    return corrected_word

def get_wordnet_pos_code(tag):
    if tag.startswith('NN'):
        return wn.NOUN
    elif tag.startswith('VB'):
        return wn.VERB
    elif tag.startswith('JJ'):
        return wn.ADJ
    elif tag.startswith('RB'):
        return wn.ADV
    else:
        return ''


#train_size = len(train)

# The following code is used for testing the model performance

sents = load_sentences("/Users/macbook/Desktop/corpora/ProofSeer/test176.txt")
print sents
#(builders[0], builders[1], WORDS_LOOKUP, W_sm, b_sm) = model.load("C:\\corpora\\adam_batch_bilstm_bigmodel2.txt")
total_mrr = 0
count_mrr = 0
for sentence in sents:
    count_mrr +=  1
    total_mrr = total_mrr + evaluate_sentence(sentence)
    print "Current MRR", total_mrr/float(count_mrr)
print "Total Mrr", total_mrr/float(len(sents))
